import os,glob
import datetime
import json
import numpy as np
import tensorflow as tf
from keras.layers import Bidirectional
from sklearn.model_selection import train_test_split
from rdp import rdp
import numpy as np

from google.colab import drive
drive.mount("/content/drive")

SAVE_DIR = "/content/drive/MyDrive/soturon/models"
os.makedirs(SAVE_DIR, exist_ok=True)
print("SAVE_DIR:", SAVE_DIR)


CLASS_NAMES = ["straight","skinny","wide","frea","jogger","shirt","long_t","jacket","coat","hoodie"]
#各クラスをid付け
label_map = {}
for i, name in enumerate(CLASS_NAMES):
  label_map[name] = i

MIN_POINTS = 10

#左上詰め、スケーリング
def simplified(strokes):
    x_list = []
    y_list = []
    for stroke in strokes:
      x_list += stroke[0]
      y_list += stroke[1]

    min_x, min_y = min(x_list), min(y_list)
    max_x, max_y = max(x_list), max(y_list)
    w = max_x - min_x
    h = max_y - min_y
    s = max(w,h)
    scale = 255.0 / s

    out = []#それぞれのストロークを順番に処理
    for stroke in strokes:
      x_list = []
      y_list = []

      #座標を左上に寄せて、255の範囲にそろえる
      for i in range(len(stroke[0])):
          x = (stroke[0][i] - min_x) * scale
          y = (stroke[1][i] - min_y) * scale
          x_list.append(x)
          y_list.append(y)
      out.append([x_list, y_list])
    return out

#eps1, eps15, eps2,eps25
def rdp_simplify_strokes(xy_strokes, eps=1.5, min_keep=10):
    out = []
    for x_list, y_list in xy_strokes:
        n = len(x_list)
        if n <= min_keep:
            out.append([x_list, y_list])
            continue

        pts = np.stack([x_list, y_list], axis=1).astype(np.float32)
        reduced = np.array(rdp(pts, epsilon=eps), dtype=np.float32)

        if len(reduced) < min_keep:
            out.append([x_list, y_list])
        else:
            out.append([reduced[:, 0].tolist(), reduced[:, 1].tolist()])
    return out

# ストローク形式の描画データ(strokes)を系列データ(seq)に変換
def convert_strokes_to_seq(strokes):
    seq_5d = []
    prev_x, prev_y = 0, 0
    for stroke in strokes:
        x_list, y_list = stroke[0], stroke[1]

        for i in range(len(x_list)):
            x, y = x_list[i], y_list[i]
            dx, dy = x - prev_x, y - prev_y

            pen = [1, 0, 0]
            if i == len(x_list) - 1:
                pen = [0, 1, 0]
            seq_5d.append([dx, dy] + pen)
            prev_x, prev_y = x, y

    seq_5d.append([0, 0, 0, 0, 1])
    return seq_5d

# 部分系列
def make_partials(seq_5d,label,rates=(0.2, 0.4, 0.6, 0.8, 1.0)):
    out = []
    n = len(seq_5d)
    if n < MIN_POINTS:
      return out
    for r in rates:
      k = int(n * r)
      if k < 1:
        continue
      out.append((seq_5d[:k], label))
    return out

def points_to_xy(strokes_pts):
    xy_strokes = []
    for stroke in strokes_pts:
        if not stroke:
            continue

        x_list, y_list = [], []
        # 各ストローク内の点を一つずつ
        for point in stroke:
            x = float(point[0])
            y = float(point[1])
            x_list.append(x)
            y_list.append(y)
        # xとyのリストを1つのストロークとしてまとめる
        xy_strokes.append([x_list, y_list])
    return xy_strokes

# ndjsonを読み込む
def load_drawings(folder_path, label=0, limit=2000):
    data = []
    files = sorted(glob.glob(f"{folder_path}/*.ndjson"))  #フォルダ内のndjsonファイルをとる

    for file in files:
        with open(file, "r") as f:
            for i, line in enumerate(f):
                if i >= limit:
                    break
                sample = json.loads(line)  # 各行(json)を辞書に変換
                raw_strokes = sample["drawing"]
                xy = points_to_xy(raw_strokes)
                sim = simplified(xy)
                sim = rdp_simplify_strokes(sim,eps=1.5)
                seq_5d = convert_strokes_to_seq(sim)
                if len(seq_5d) < MIN_POINTS:
                    continue
                data.extend(make_partials(seq_5d, label))
    return data

# ファイル読み込み
straight_data = load_drawings("/content/pants_data/N_STRAIGHT/n_STRAIGHT", label=0)
skinny_data = load_drawings("/content/pants_data/N_SKINNY/n_SKINNY", label=1)
wide_data = load_drawings("/content/pants_data/N_WIDE/n_WIDE", label=2)
frea_data = load_drawings("/content/pants_data/N_FREA/N_FREA", label=3)
jogger_data = load_drawings("/content/pants_data/N_JOGGER/n_JOGGER", label=4)
shirt_data = load_drawings("/content/pants_data/N_SHIRT/N_SHIRT", label=5)
long_t_data = load_drawings("/content/pants_data/N_LONG_T/N_LONG_T", label=6)
jacket_data = load_drawings("/content/pants_data/N_JACKET/N_JACKET", label=7)
coat_data = load_drawings("/content/pants_data/N_COAT/N_COAT", label=8)
hoodie_data = load_drawings("/content/pants_data/N_HOODIE/N_HOODIE", label=9)


all_data = straight_data + skinny_data + wide_data + frea_data + jogger_data + shirt_data + long_t_data + hoodie_data + coat_data + jacket_data

print("straight:", len(straight_data))
print("skinny:", len(skinny_data))
print("wide:", len(wide_data))
print("frea:", len(frea_data))
print("jogger:", len(jogger_data))
print("shirt:", len(shirt_data))
print("long_t:", len(long_t_data))
print("jacket:", len(jacket_data))
print("coat:", len(coat_data))
print("hoodie:", len(hoodie_data))
print("all_data:", len(all_data))

# パディング
MAXLEN = 100

#それ以上は切る、足りない部分は0パディング
X, Y = [], []
for seq_5d,label in all_data:
  if len(seq_5d) < MAXLEN:
    seq_5d += [[0.0]*5]*(MAXLEN - len(seq_5d))
  else:
    seq_5d = seq_5d[:MAXLEN]

  X.append(seq_5d)
  Y.append(label)

# numpy配列に変換
X = np.array(X, dtype=np.float32)
Y = np.array(Y, dtype=np.int32)

# 8:2に分割)(学習用、テスト用データ)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)

# モデルの構築
model = tf.keras.Sequential([
    tf.keras.Input(shape=(MAXLEN, 5)),
    tf.keras.layers.Masking(mask_value=0.0),
    Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),
    tf.keras.layers.Dropout(0.3),
    Bidirectional(tf.keras.layers.LSTM(128)),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(
    optimizer="adam",
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

# モデル学習を開始
history = model.fit(
    X_train, Y_train,
    validation_data=(X_test, Y_test),
    epochs=50
    )

# 評価
loss, acc = model.evaluate(X_test, Y_test)
print("Accuracy =", acc)
print("Loss =", loss)

timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
accuracy_str = f"{acc*100:.2f}".replace('.', '_')

#保存
model_filename = f"qd_apparel_acc{accuracy_str}_{timestamp}.keras"
log_filename = model_filename.replace(".keras", ".txt")

model_path = os.path.join(SAVE_DIR, model_filename)
log_path = os.path.join(SAVE_DIR, log_filename)

# 結果をファイルに保存
with open(log_path, "w") as f:
    f.write(f"Loss: {loss:.4f}\n")
    f.write(f"Accuracy: {acc:.4f}\n")
print("log saved:", log_path)

# 学習したモデルを保存
model.save(model_path)
print("model saved:", model_path)




import matplotlib.pyplot as plt
import numpy as np

hist_dict = history.history

epochs = range(1, len(hist_dict['loss']) + 1)

#Lossグラフ
plt.figure(figsize=(15, 5))
plt.subplot(1, 2, 1)

# 訓練データと検証データの損失をプロット
plt.plot(epochs, hist_dict['loss'], 'b', label='Training Loss')
plt.plot(epochs, hist_dict['val_loss'], 'r', label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)


# Accuracyグラフ
plt.subplot(1, 2, 2)

# 訓練データと検証データの精度をプロット
plt.plot(epochs, hist_dict['accuracy'], 'b', label='Training Accuracy')
plt.plot(epochs, hist_dict['val_accuracy'], 'r', label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

plt.tight_layout(rect=[0,0,1,0.92])

fig_path = os.path.join(SAVE_DIR, model_filename.replace(".keras", "_acc_loss.png"))
plt.savefig(fig_path, dpi=200, bbox_inches="tight")
print("figure saved:", fig_path)

plt.show()

print("グラフの生成を完了しました。")
